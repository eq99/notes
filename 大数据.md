# 安装 hadoop

准备 java 环境

```shell
$ apt update && apt upgrade
$ apt install openjdk-8-jdk
```

下载 hadoop：https://dlcdn.apache.org/hadoop/common/

```shell
$ scp /local/ root@remote:/root/

# ssh root@remote
$ tar -xvzf hadoop-3.0.1.tar.gz
$ mv hadoop-3.0.1.tar.gz hadoop
```

生成公钥

```shell
$ ssh -keygen -t rsa
$ ssh-copy-id root@localhost
```

[伪分布式配置](https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html#Pseudo-Distributed_Operation)

为 hadoop 配置 java：`hadoop/etc/hadoop/hadoop-env.sh`

```shell
export JAVA_HOME=$(readlink -f /usr/bin/java | sed "s:bin/java::")
```

`etc/hadoop/core-site.xml`

```xml
<configuration>
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://localhost:9000</value>
    </property>
</configuration>
```

`etc/hadoop/hdfs-site.xml`

```xml
<configuration>
    <property>
        <name>dfs.replication</name>
        <value>1</value>
    </property>
</configuration>
```

格式化 hdfs

```shell
$ bin/hdfs namenode -format
```

启动 dfs:

```shell
$ sbin/start-dfs.sh
```

如果报错：

```shell
ERROR: Attempting to operate on hdfs datanode as root
ERROR: but there is no HDFS_DATANODE_USER defined. Aborting operation.
```

可以添加配置如下环境变量：

```shell
export PATH=$PATH:/root/hadoop/bin
# https://stackoverflow.com/a/48170409/17225183
export HDFS_NAMENODE_USER="root"
export HDFS_DATANODE_USER="root"
export HDFS_SECONDARYNAMENODE_USER="root"
export YARN_RESOURCEMANAGER_USER="root"
export YARN_NODEMANAGER_USER="root"
```

然后执行：

```shell
$ source .bashrc
```

最后测试 hadoop

```shell
hdfs dfs -mkdir /user
hdfs dfs -mkdir /user/root
hdfs dfs -ls /
# hadoop fs -ls /

hdfs dfs -mkdir input
echo "hello">hello.txt
hdfs dfs -put hello.txt input
hdfs dfs -cat input/hello.txt
```

大功告成。



# 安装 HBase

下载页面：https://hbase.apache.org/downloads.html

```shell
tar -xvzf hbase-$VER-bin.tar.gz
mv hbase-$VER-bin.tar.gz hbase
```

配置 java 路径：`HBase/conf/hbase-env.sh`

```shell
export JAVA_HOME=$(readlink -f /usr/bin/java | sed "s:bin/java::")
```

添加环境变量：

```shell
export HADOOP_HOME=/root/hadoop
export HBASE_HOME=/root/hbase
export PATH=$PATH:$HADOOP_HOME/bin:$HBASE_HOME/bin
# https://stackoverflow.com/a/48170409/17225183
export HDFS_NAMENODE_USER="root"
export HDFS_DATANODE_USER="root"
export HDFS_SECONDARYNAMENODE_USER="root"
export YARN_RESOURCEMANAGER_USER="root"
export YARN_NODEMANAGER_USER="root"
```

配置伪分布式：`conf/hbase-site.xml`

```xml
<configuration>
<property>
  <name>hbase.cluster.distributed</name>
  <value>true</value>
</property>
<property>
  <name>hbase.rootdir</name>
  <value>hdfs://localhost:9000/hbase</value>
</property>
<configuration>
```

查看 hdfs 端口命令：

```shell
hdfs getconf -confKey fs.defaultFS
```

文档：http://hbase.apache.org/book.html#quickstart_pseudo

启动：

```shell
start-hbase.sh 
```

查看文件：

```shell
hadoop fs -ls /hbase
```

使用 HBase Shell

```shell
hbase shell
>list
>create 'student','Sname','Ssex','Sage','Sdept','course'
>put 'student','95001','Sname','LiYing'
>get 'student','95001'

# 获取全表数据
>scan 'student'

# 删除记录
>delete 'student','95001','Ssex'
>deleteall 'student','95001'

# 删除表
>disable 'student'
>drop 'student'
```

参考文章：https://zhuanlan.zhihu.com/p/131626000

